{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39da35dc",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbc884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "import json\n",
    "pipe = pipeline(\"text-generation\", model=\"LiquidAI/LFM2-350M\")\n",
    "\n",
    "def speak(query: str, messages = []) -> list: # speak to LLM\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "    response = pipe(messages)\n",
    "    messages = response[0][\"generated_text\"]\n",
    "    print(json.dumps(messages, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "class Conversation:\n",
    "    \"\"\"\n",
    "    Hold conversation with language model\n",
    "    \"\"\"\n",
    "    def __init__(self, model = \"google/flan-t5-base\"):\n",
    "        self.pipe = pipeline(\"text-generation\", model=model, max_new_tokens = 200)\n",
    "        self.messages = []\n",
    "        return None\n",
    "    \n",
    "    def speak(self,query: str) -> list: # speak to LLM\n",
    "        self.messages.append({\"role\": \"user\", \"content\": query})\n",
    "        response = self.pipe(self.messages)\n",
    "        self.messages = response[0][\"generated_text\"]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7de909d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8d0f9374244138af86a04ce6b9586d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80860f07b683430081c661f87fe7e141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608877ac964142a79e9e52589e9f5453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cefe8a3791410386344ed82bc6878d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c148cd10e04feea72acbb5d87575e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ba32a9649943309bd4cb6f4ecaa715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56df1dc0fa74b8c82b38757e2a3faab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m convo = Conversation()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mconvo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mConversation.speak\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mspeak\u001b[39m(\u001b[38;5;28mself\u001b[39m,query: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m: \u001b[38;5;66;03m# speak to LLM\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m.messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: query})\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m.messages = response[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:314\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         chats = (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1458\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1451\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1452\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1455\u001b[39m         )\n\u001b[32m   1456\u001b[39m     )\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1464\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1465\u001b[39m     model_outputs = \u001b[38;5;28mself\u001b[39m.forward(model_inputs, **forward_params)\n\u001b[32m   1466\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:350\u001b[39m, in \u001b[36mTextGenerationPipeline.preprocess\u001b[39m\u001b[34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m continue_final_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    349\u001b[39m         continue_final_message = prompt_text.messages[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    359\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(prefix + prompt_text, return_tensors=\u001b[38;5;28mself\u001b[39m.framework, **tokenizer_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1620\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1617\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1618\u001b[39m     tokenizer_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m chat_template = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conversation, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   1623\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(conversation[\u001b[32m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conversation[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1624\u001b[39m ):\n\u001b[32m   1625\u001b[39m     conversations = conversation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1742\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.get_chat_template\u001b[39m\u001b[34m(self, chat_template, tools)\u001b[39m\n\u001b[32m   1740\u001b[39m         chat_template = \u001b[38;5;28mself\u001b[39m.chat_template\n\u001b[32m   1741\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1743\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1744\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33margument was passed! For information about writing templates and setting the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1745\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1746\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1747\u001b[39m         )\n\u001b[32m   1749\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[31mValueError\u001b[39m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "convo = Conversation()\n",
    "convo.speak(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I help you today?'}]\n"
     ]
    }
   ],
   "source": [
    "print(convo.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.speak(\"I want to play a game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd5e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "Hello! How can I help you today?\n",
      "I want to play a game\n",
      "That sounds fun! There are many games you can play, but here are a few popular ones:\n",
      "\n",
      "1. **Catan**: A simulation game where you build cities, transport goods, and trade with other players to make the most profitable settlements.\n",
      "\n",
      "2. **Codenames**: A word association game where players work in teams to guess hidden words.\n",
      "\n",
      "3. **Katan**: Another name for Catan, this is a strategy board game similar to the one I mentioned, focusing on trading and resource management.\n",
      "\n",
      "4. **Minecraft**: A sandbox game where you build and explore a virtual world using various resources.\n",
      "\n",
      "5. **Pandemic**: A zombie-shootdown survival game where you work together to stop the spread of deadly diseases in different countries.\n",
      "\n",
      "6. **Portal**: A puzzle-platformer that requires solving spatial puzzles to open portals and escape doors.\n",
      "\n",
      "7. **Ticket to Ride**: A land management game where you plot train routes across a continent to claim territories.\n",
      "\n",
      "8. **Risk**: A card game where players try to prevent a bank run from crashing a national currency.\n",
      "\n",
      "Which game sounds interesting to you? Or do you have a\n"
     ]
    }
   ],
   "source": [
    "for message in convo.messages:\n",
    "    print(message[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
